# -*- coding: utf-8 -*-
"""ML_Talents

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X6-kTW1Kcm57pdGaj-C0ykUoncjxwUWv
"""

!pip install pymorphy2

import os
import re
from collections import defaultdict
import pickle
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from functools import lru_cache
from tqdm import tqdm
import gensim
import pymorphy2
from transformers import MBartTokenizer, MBartForConditionalGeneration
from google.colab import drive
import string
import json
pd.set_option('display.max_columns', 600)

morph = pymorphy2.MorphAnalyzer()

def parse_work(work):
    work_text = f"Дата начала работы: {work['starts']}\n"
    work_text += f"Дата окончания работы: {work['ends']}\n"
    work_text += f"Работодатель: {work['employer']}\n"
    work_text += f"Город: {work['city']}\n"
    work_text += f"Позиция: {work['position']}\n"
    work_text += f"Описание: {work['description']}\n"
    return work_text

def parse_education(education):
    work_text = f"Год обучения: {education['year']}\n"
    work_text += f"Образовательная организация: {education['organization']}\n"
    work_text += f"Обучение: {education['faculty']}\n"
    work_text += f"Направление: {education['specialty']}\n"
    work_text += f"Результат обучения: {education['result']}\n"
    work_text += f"Тип обучения: {education['education_type']}\n"
    work_text += f"Уровень обучения: {education['education_level']}\n"
    return work_text

def parse_resume(resume):
    resume_text = f"ФИО: {resume['first_name']} {resume['last_name']}\n"
    resume_text += f"Дата рождения: {resume['birth_date']}\n"
    resume_text += f"Страна: {resume['country']}\n"
    resume_text += f"Город: {resume['city']}\n"
    if not isinstance(resume['about'], type(None)):
        resume_text += f"Описание: {resume['about']}\n"
    resume_text += f"Ключевые навыки: {resume['key_skills']}\n"
    resume_text += f"Опыт работы:\n"
    if "experienceItem" in resume:
        for idx, work in enumerate(resume["experienceItem"]):
            resume_text += f"Работа номер {idx}\n"
            resume_text += parse_work(work)
    if "educationItem" in resume:
        for idx, education in enumerate(resume["educationItem"]):
            resume_text += f"Обучение номер {idx}\n"
            resume_text += parse_education(education)
    resume_text += f"Владение языками:\n"
    if "languageItems" in resume:
        for language in resume["languageItems"]:
            resume_text += f"{language}\n"
    return resume_text

data_path = "/content/case_2_reference_without_resume_sorted.json"
data_json = [json.load(open(data_path, 'r', encoding='utf-8'))]
dataset_dict = {"id_vacancy":[], "id_resume":[],"description":[], "description_v":[]}
for data in data_json:
    vacancy = data["vacancy"]
    vacancy_id = vacancy["uuid"]
    vacancy_text = f"{vacancy['description']}"
    print(vacancy_text)
    resumes = data["resumes"]
    for resume in resumes:
        dataset_dict["id_vacancy"].append(vacancy_id)
        dataset_dict["id_resume"].append(resume["uuid"])
        if "experienceItem" in resume:
          expirience = ' '.join([exp['description'] if not isinstance(exp['description'], type(None)) else "" for exp in resume['experienceItem']])
        dataset_dict["description"].append(f"{resume['key_skills']} {resume['about'] if not isinstance(resume['about'], type(None)) else ''} {expirience}")
        dataset_dict["description_v"].append(vacancy_text)

"""# Создаём таблицу"""

# Создаем DataFrame
df = pd.DataFrame(dataset_dict)

# Выводим DataFrame
df

vacancies = set(df.id_vacancy.tolist())
vacancies = list(vacancies)
vacancies

data = df.loc[df['id_vacancy']==vacancies[0]]
data

with open('stopwords-ru.txt', 'r') as f:
    stop_words = f.read().split('\n')
len(stop_words)

stop_words_ = """дабы, лишь только, таким, для, на, по, со, из, от, до, без, над, под, за, при, после, во, же, то, бы, всего, итого, даже, да, нет, ой, ого, эх, браво, здравствуйте, спасибо, извините, скажем, может, допустим, честно говоря, например, на самом деле, однако, вообще, в, общем, вероятно, очень, минимально, максимально, абсолютно, огромный, предельно, сильно, слабо, самый, сайт, давать, всегда, однако, и, а, но, да, если, что, когда, потому, что, так, как, как, будто, вследствие, того, что, с, тех, пор, как, в, то, время, как, для, того, чтобы, ни, то, ли, но, зато, от, и, к, система, сотрудник, компания, проект, стек"""
len(stop_words_)

stop_words_1  = set(stop_words + stop_words_.replace('\n','').split(', '))
print(stop_words_1)

len(stop_words_1)

for i in range (len(vacancies)):
  data = df.loc[df['id_vacancy'] == vacancies[i]]
  texts = []
  content_values = data['description'].tolist()
  for value in content_values:
    texts.append(value)
  def clear_text(t):
    t = str(t).lower()
    t = t.replace('\n', '  ')
    t = t.replace('.', '. ')
    t = t.replace(',', ', ')
    t = t.replace('ха0', ' ')
    return ' '.join(re.findall('[a-za-яё]+', t))
  texts_cleared = []
  for text in texts:
    texts_cleared.append(clear_text(text))
  texts_tokenized = [t.split() for t in texts_cleared]
  texts_tokenized[-len(texts_tokenized)]
  @lru_cache(100000)
  def lemmatize(s):
    s = str(s).lower()
    return morph.parse(s)[0].normal_form
  texts_tokenized_1 = [[lemmatize(tt) for tt in t if len(tt) > 1]
                     for t in tqdm(texts_tokenized)]
  texts_tokenized_1 = [[tt for tt in t if tt not in stop_words_1]
                     for t in texts_tokenized_1]
  texts_tokenized_1[-len(texts_tokenized_1)]
  ttfs = defaultdict(int)
  dfs = defaultdict(int)
  for t in texts_tokenized_1:
    for term in set(t):
        if len(term) > 1:
            ttfs[term] += t.count(term)
            dfs[term] += 1
  df_dfs = pd.DataFrame({'cnt_dfs': pd.Series(dfs), 'cnt_ttfs': pd.Series(ttfs)})
  df_dfs.index.name = 'term'
  df_dfs = df_dfs.reset_index()
  df_dfs = df_dfs.sort_values('cnt_ttfs', ascending=False)
  df_dfs_1 = df_dfs[(df_dfs['cnt_ttfs'] > 1)]
  dfs_1 = df_dfs_1.to_dict('records')
  dfs_1 = {i['term']: i['cnt_dfs'] for i in dfs_1}
  terms = set(dfs_1.keys())
  texts_tokenized_2 = [[tt for tt in t if tt in terms] for t in texts_tokenized_1]
  #%%time

  model = gensim.models.FastText(texts_tokenized_2, min_count=1, negative=5,
                               vector_size=100, window=5, workers=16)
  vars(model.wv)
  model.wv.vectors_vocab = None
  model.wv.vectors = model.wv.vectors.astype(np.float16)
  model.wv.vectors_ngrams = model.wv.vectors_ngrams.astype(np.float16)

  def text_vec(t):
    res = np.zeros(100)
    cnt = 0
    for tt in t[:60]:
        try:
            res += model.wv[tt]
            cnt += 1
        except:
            pass
    if cnt > 0:
        return res / (cnt * np.linalg.norm(res / cnt))
    else:
        return res
  np.linalg.norm(text_vec(texts_tokenized_2[0]))
  texts_mean_vect = [text_vec(t) for t in tqdm(texts_tokenized_2)]
  np_texts_mean_vect = np.vstack(texts_mean_vect)

  #query = 'Описание Мы расширяем команды и ищем разработчиков для развития нескольких сервисов:   Инвестиции. Мы — лидер среди брокеров по количеству активных клиентов. Делаем инвестиции удобными, технологичными и понятными;   Бизнес. Меняем подход к ведению бухгалтерии, обмену документами между организациями и работе с архивами — переводим все и вся в цифру;   Страхование. Развиваем платформу прямых продаж. Наша цель — предсказуемый, удобный процесс для бизнеса и клиента; Платежные технологии и процессинг. Занимаемся разработкой и поддержкой платежного шлюза банка. Задачи шлюза — определять тип платежей, наполнять их данными из других систем банка и проверять разрешенность операций. У нас много интересных и разнообразных задач, опытная команда и отличные возможности для роста. Откликайтесь на вакансию, чтобы узнать о проектах и выбрать подходящий для вас. Требования Опыт разработки на Java от 3 лет Опыт коммерческой разработки на Java 11+ или Kotlin Опыт коммерческой разработки с любым из фреймворков: Spring Boot, Quarkus, Micronaut или Vert.x Опыт коммерческой разработки с одним из контейнеризаторов: Kubernetes, Docker или OpenShift Опыт коммерческой разработки с одним из брокеров: Kafka, Rabbit MQ или Active MQ Опыт коммерческой разработки с Postgress, MySQL или Oracle будет плюсом Опыт работы с системой контроля версий Мы предлагаем Работу в офисе или удаленно — по договоренности Возможность работы в аккредитованной ИТ-компании Платформу обучения и развития «  Апгрейд». Курсы, тренинги, вебинары и базы знаний. Поддержку менторов и наставников, помощь в поиске точек роста и карьерном развитии Заботу о здоровье. Оформим полис ДМС со стоматологией и страховку от несчастных случаев. Предложим льготное страхование вашим близким Бесплатный фитнес-зал или компенсацию затрат на спортивные занятия 3 дополнительных дня отпуска в год Уникальную well-being-программу, направленную на физическое и ментальное благополучие сотрудников Достойную зарплату — обсудим ее на собеседовании'
  query = data['description_v']
  query = clear_text(query)
  vect_query = text_vec(query.split()).reshape(-1,1) / np.linalg.norm(text_vec(query.split()))
  np.linalg.norm(vect_query)

  relevance = np.matmul(np_texts_mean_vect, vect_query)
  top_ind = (-relevance.flatten()).argsort()
  print(top_ind)
  print(relevance[top_ind])

import csv
for i in top_ind[:10]:
  print(df["id_resume"][i])
with open("ans.csv", "w",newline='',encoding="utf-8") as f:
  writer = csv.writer(f)
  field = ["resume_uuid"]
  writer.writerow(field)
  for i in top_ind[:10]:
    writer.writerow([str(df["id_resume"][i])])